{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dengue-anvil_classifier\n",
    "This notebook uses data from JSON delivered on October 2019 (dengue.json). To speed up retrieval and because of errors in the original file, we used a filtered database from a MongoDB collection (ufmg_filtered).\n",
    "\n",
    "The notebook delivers 2 jsons: \n",
    "* tweets_filtered.json: A file even more filtered from the MongoDB to save memory usage.\n",
    "* tweets_anvil_input.json: input for Anvil database which was used for data annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../inputs/'\n",
    "outputs = '../outputs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# configure MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import dns\n",
    "client = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.twitter_fixed\n",
    "collection_ufmg = db.ufmg_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrieve data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% done\n"
     ]
    }
   ],
   "source": [
    "file_len = 7503436\n",
    "objects = collection_ufmg.find({})\n",
    "tweets_list = []\n",
    "data = pd.DataFrame()\n",
    "count = 0\n",
    "for obj in objects:\n",
    "    date = obj['date']\n",
    "    if isinstance(date, int):\n",
    "        date = date/1000\n",
    "    else: date = time.mktime(time.strptime(date[:10], '%Y-%m-%d'))\n",
    "    if date >= 1451617260: # 1451617260 = 2016-01-01\n",
    "        obj['date'] = time.strftime('%Y-%m-%d', time.localtime(date))\n",
    "        obj['text'] = re.sub(r'\\\\', '', obj['text'])\n",
    "        if 'extended_tweet' in obj: \n",
    "            obj['extended_tweet'] = re.sub(r'\\\\', '', obj['extended_tweet']['full_text'])\n",
    "        tweets_list.append(obj)\n",
    "\n",
    "    count += 1\n",
    "    if count % (int(file_len/1000)) == 0:\n",
    "        \n",
    "        #here I reset the list to save memory usage\n",
    "        if tweets_list: data = data.append(tweets_list, ignore_index=True)\n",
    "        tweets_list = []\n",
    "        \n",
    "        clear_output()\n",
    "        \n",
    "        frac = count/file_len*100\n",
    "        print(\"%.1f\" % frac, \"% done\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter by date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://portalarquivos2.saude.gov.br/images/pdf/2018/agosto/21/Publicacao-BE-2018-SE-30.pdf\n",
    "\n",
    "* zika\n",
    "    * peak year = 2016 \n",
    "    * peak week = week 7 = 14-21/2/2016\n",
    "    * occurrence timeframe = 7-14 = 14/2 a 9/4\n",
    "* chikungunya\n",
    "    * peak year = 2017 \n",
    "    * peak week = week 17 = 23-30/4/2017\n",
    "    * occurrence timeframe = 10-17 = 5/3 a 29/4\n",
    "* dengue\n",
    "    * peak year = 2016, but e used 2018 to add variability from other years\n",
    "    * occurrence timeframe = 14-21 = 1/4 a 26/5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter by 7 weeks  timespan on each year\n",
    "Those weeks correspond to a peak for each virus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data['date'] >= '2016-02-14 00:00:01') & (data['date'] <= '2016-04-09 23:59:59') \\\n",
    "                 | (data['date'] >= '2017-03-05 00:00:01') & (data['date'] <= '2017-04-29 23:59:59') \\\n",
    "                 | (data['date'] >= '2018-04-01 00:00:01') & (data['date'] <= '2018-05-27 23:59:59') \\\n",
    "    ]\n",
    "data = data[data['lang'] == 'pt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>class_campanha</th>\n",
       "      <th>class_exp_pessoal</th>\n",
       "      <th>class_informacao</th>\n",
       "      <th>class_opiniao</th>\n",
       "      <th>class_parodia</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>date</th>\n",
       "      <th>extended_tweet</th>\n",
       "      <th>lang</th>\n",
       "      <th>location</th>\n",
       "      <th>place</th>\n",
       "      <th>screen_name</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3366784</th>\n",
       "      <td>1000931012069724160</td>\n",
       "      <td>0.031485</td>\n",
       "      <td>0.504642</td>\n",
       "      <td>0.111274</td>\n",
       "      <td>0.200243</td>\n",
       "      <td>0.152354</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>na merda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bwliebar_</td>\n",
       "      <td>minha irmã brigando pq eu postei que Deus é zi...</td>\n",
       "      <td>2551219417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366785</th>\n",
       "      <td>1000933497240981507</td>\n",
       "      <td>0.114587</td>\n",
       "      <td>0.104780</td>\n",
       "      <td>0.085701</td>\n",
       "      <td>0.220957</td>\n",
       "      <td>0.473975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-27</td>\n",
       "      <td>@Crente_Quadrado preciso conversar com noé sob...</td>\n",
       "      <td>pt</td>\n",
       "      <td>Paraíso do Norte, Brasil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bielrobati</td>\n",
       "      <td>@Crente_Quadrado preciso conversar com noé sob...</td>\n",
       "      <td>180148246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366786</th>\n",
       "      <td>1000934479580590080</td>\n",
       "      <td>0.064492</td>\n",
       "      <td>0.575904</td>\n",
       "      <td>0.160251</td>\n",
       "      <td>0.077597</td>\n",
       "      <td>0.121755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>São Leopoldo, Brasil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CarLouhs</td>\n",
       "      <td>Mano, que dor no meu pescoço acho que é dengue</td>\n",
       "      <td>949441490790551554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366787</th>\n",
       "      <td>1000934567551922176</td>\n",
       "      <td>0.036469</td>\n",
       "      <td>0.071213</td>\n",
       "      <td>0.560594</td>\n",
       "      <td>0.117393</td>\n",
       "      <td>0.214331</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gds506</td>\n",
       "      <td>Zancudo Aedes aegypti genera resistencia a ins...</td>\n",
       "      <td>15686478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366788</th>\n",
       "      <td>1000934642193780738</td>\n",
       "      <td>0.013150</td>\n",
       "      <td>0.417638</td>\n",
       "      <td>0.247061</td>\n",
       "      <td>0.086871</td>\n",
       "      <td>0.235281</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-05-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pt</td>\n",
       "      <td>São Paulo, Brasil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>duda_senam</td>\n",
       "      <td>RT @fluminenseraiz: VAI TOMAR NO CU O FLUMINEN...</td>\n",
       "      <td>532225289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         _id  class_campanha  class_exp_pessoal  \\\n",
       "3366784  1000931012069724160        0.031485           0.504642   \n",
       "3366785  1000933497240981507        0.114587           0.104780   \n",
       "3366786  1000934479580590080        0.064492           0.575904   \n",
       "3366787  1000934567551922176        0.036469           0.071213   \n",
       "3366788  1000934642193780738        0.013150           0.417638   \n",
       "\n",
       "         class_informacao  class_opiniao  class_parodia coordinates  \\\n",
       "3366784          0.111274       0.200243       0.152354         NaN   \n",
       "3366785          0.085701       0.220957       0.473975         NaN   \n",
       "3366786          0.160251       0.077597       0.121755         NaN   \n",
       "3366787          0.560594       0.117393       0.214331         NaN   \n",
       "3366788          0.247061       0.086871       0.235281         NaN   \n",
       "\n",
       "               date                                     extended_tweet lang  \\\n",
       "3366784  2018-05-27                                                NaN   pt   \n",
       "3366785  2018-05-27  @Crente_Quadrado preciso conversar com noé sob...   pt   \n",
       "3366786  2018-05-27                                                NaN   pt   \n",
       "3366787  2018-05-27                                                NaN   pt   \n",
       "3366788  2018-05-27                                                NaN   pt   \n",
       "\n",
       "                         location place screen_name  \\\n",
       "3366784                  na merda   NaN   Bwliebar_   \n",
       "3366785  Paraíso do Norte, Brasil   NaN  bielrobati   \n",
       "3366786      São Leopoldo, Brasil   NaN    CarLouhs   \n",
       "3366787                Costa Rica   NaN      gds506   \n",
       "3366788         São Paulo, Brasil   NaN  duda_senam   \n",
       "\n",
       "                                                      text             user_id  \n",
       "3366784  minha irmã brigando pq eu postei que Deus é zi...          2551219417  \n",
       "3366785  @Crente_Quadrado preciso conversar com noé sob...           180148246  \n",
       "3366786     Mano, que dor no meu pescoço acho que é dengue  949441490790551554  \n",
       "3366787  Zancudo Aedes aegypti genera resistencia a ins...            15686478  \n",
       "3366788  RT @fluminenseraiz: VAI TOMAR NO CU O FLUMINEN...           532225289  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(len(data))\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encode text\n",
    "Necessary to allow its insertion on anvil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['minha irmã brigando pq eu postei que Deus é zika kkkkk \"respeita, pq ele é nobre\"',\n",
       " '@Crente_Quadrado preciso conversar com noé sobre ter deixado os mosquitos entrarem\\nculpa dele temos chigungunha, de… https://t.co/h2NssSI9kT',\n",
       " 'Mano, que dor no meu pescoço acho que é dengue',\n",
       " 'Zancudo Aedes aegypti genera resistencia a insecticidas https://t.co/HXojBli9K6',\n",
       " 'RT @fluminenseraiz: VAI TOMAR NO CU O FLUMINENSE É FODA, GRAÇAS À DEUS A ZIKA CONTRA CHAPE FOI EMBORA, COMO O FRED JÁ DIZIA \"TÁ SAINDO UM M…']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.tail()['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_char(text):\n",
    "    text = re.sub('[áàãâ]', 'a', text)\n",
    "    text = re.sub('[óòõô]', 'o', text)\n",
    "    text = re.sub('[éèê]', 'e', text)\n",
    "    text = re.sub('[íì]', 'i', text)\n",
    "    text = re.sub('[úù]', 'u', text)\n",
    "    text = re.sub('ç', 'c', text)\n",
    "    text = re.sub('[ÁÀÃÂ]', 'A', text)\n",
    "    text = re.sub('[ÓÒÕÔ]', 'O', text)\n",
    "    text = re.sub('[ÉÈÊ]', 'E', text)\n",
    "    text = re.sub('[ÍÌ]', 'I', text)\n",
    "    text = re.sub('[ÚÙ]', 'U', text)\n",
    "    text = re.sub('Ç', 'C', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_original']  = data['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text_original'].apply(lambda x: remove_special_char(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['minha irma brigando pq eu postei que Deus e zika kkkkk \"respeita, pq ele e nobre\"',\n",
       " '@Crente_Quadrado preciso conversar com noe sobre ter deixado os mosquitos entrarem\\nculpa dele temos chigungunha, de… https://t.co/h2NssSI9kT',\n",
       " 'Mano, que dor no meu pescoco acho que e dengue',\n",
       " 'Zancudo Aedes aegypti genera resistencia a insecticidas https://t.co/HXojBli9K6',\n",
       " 'RT @fluminenseraiz: VAI TOMAR NO CU O FLUMINENSE E FODA, GRACAS A DEUS A ZIKA CONTRA CHAPE FOI EMBORA, COMO O FRED JA DIZIA \"TA SAINDO UM M…']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data.tail()['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['_id', 'text', 'text_original', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json = data.to_json(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(outputs,'tweets_filtered.json')\n",
    "json.dump(json.loads(data_json), open(file, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter by 7 weeks  timespan on each year\n",
    "Those weeks correspond to a peak for each virus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(outputs,'tweets_filtered.json')\n",
    "data = pd.read_json(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: x.encode('ascii', errors='ignore').decode('ascii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053741\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_section = data[(data['date'] >= '2016-02-14 00:00:01') & (data['date'] <= '2016-04-09 23:59:59')].sample(10000)\n",
    "data_section = data_section.append(data[(data['date'] >= '2017-03-05 00:00:01') & (data['date'] <= '2017-04-29 23:59:59')].sample(10000))\n",
    "data_section = data_section.append(data[(data['date'] >= '2018-04-01 00:00:01') & (data['date'] <= '2018-05-27 23:59:59')].sample(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "data = data_section\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save a sample as input for training\n",
    "We need 5000 samples. First I get 6000 to remove possible duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data.sample(6000)\n",
    "sample.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
    "sample = sample.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_object = []\n",
    "count = 0\n",
    "for index, row in sample.iterrows():\n",
    "    sample_dict = {}\n",
    "    sample_dict['id'] = row['_id']\n",
    "    if isinstance(row['extended_tweet'], str):\n",
    "        text = re.sub('\\n+', ' ', row['extended_tweet'])\n",
    "        #print(text)\n",
    "    else:\n",
    "        text = re.sub('\\n+', ' ', row['text'])\n",
    "    sample_dict['message'] = text\n",
    "    sample_dict['count'] = count\n",
    "    tweets_object.append(sample_dict)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = os.path.join(outputs, 'tweets_anvil_input.json')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "json.dump(tweets_object, open(file, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluate sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1770\n",
      "1723\n",
      "1507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\Marcelo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n",
      "D:\\Users\\Marcelo\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "print(len(sample[(sample['date'] >= '2016-02-14 00:00:01') & (sample['date'] <= '2016-04-09 23:59:59')]))\n",
    "print(len(sample[(sample['date'] >= '2017-03-05 00:00:01') & (data['date'] <= '2017-04-29 23:59:59')]))\n",
    "print(len(sample[(sample['date'] >= '2018-04-01 00:00:01') & (data['date'] <= '2018-05-27 23:59:59')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample[sample.duplicated(['text'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If nothing was printed, all tweets are identified as in portuguese language\n"
     ]
    }
   ],
   "source": [
    "for index, row in sample.iterrows(): \n",
    "    if row[9] != 'pt': print(row[9], '-----', row[1])\n",
    "print('If nothing was printed, all tweets are identified as in portuguese language')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check id that should have extended_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_len = 7503436\n",
    "obj = collection_ufmg.find_one({\"_id\": 718277523306594300})\n",
    "print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_object = json.load(open(file, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Charbrevolution https://t.co/vwZsyRqyAz https://t.co/RSU6IJsNTZ https://t.co/kklzwhHCWU https://t.co/Xzji1Oj3GY https://t.co/7B4q8bN3UC https://t.co/Bwq4Zxf4h5 https://t.co/zLs9DOMLVY  https://t.co/R0ZjWz5WxZ'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_object[0]['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_texts = []\n",
    "long_texts = []\n",
    "count_long = 0\n",
    "count_short = 0\n",
    "for obj in tweets_object:\n",
    "    text = obj['message']\n",
    "    if len(text)>140: \n",
    "        count_long += 1\n",
    "        long_texts.append([obj['id'],text])\n",
    "    elif len(text)>=137 and len(text)<=140: \n",
    "        count_short += 1\n",
    "        cut_texts.append([obj['id'],text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "print(count_short)\n",
    "print(count_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
