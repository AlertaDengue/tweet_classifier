{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando modelagem de assuntos ao DHBB\n",
    "Neste capítulo vamos explorar ferramentas de modelagem de assuntos e explorar aplicações ao DHBB. Como sempre, começamos com alguns imports familiares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, pickle\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from sqlalchemy import create_engine\n",
    "from dhbbmining import *\n",
    "import ipywidgets as widgets\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos também carregar o modelo de NLP para a língua portuguesa do Spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora faremos alguns imports novos, particularmente da biblioteca [Gensim](https://radimrehurek.com/gensim), que nos oferece as ferramentas que necessitamos para modelagem de assuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from gensim.models import Word2Vec, word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para minimizar o uso de memória, vamos construir uma classe para representar o nosso corpus como um iterador, operando diretamente do banco de dados. Desta forma, ao fazer nossas análises, podemos carregar um documento por vez para alimentar os modelos, sem a necessidade de manter todo o corpus na memória, economizando memória RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrieve data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import dns\n",
    "client = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.twitter\n",
    "collection_ufmg = db.ufmg_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "file_len = 7503436\n",
    "objects = collection_ufmg.find({})\n",
    "tweets_list = []\n",
    "data = pd.DataFrame()\n",
    "count = 0\n",
    "for obj in objects:\n",
    "    date = obj['date']\n",
    "    if isinstance(date, int):\n",
    "        date = date/1000\n",
    "    else: date = time.mktime(time.strptime(date[:10], '%Y-%m-%d'))\n",
    "    if date >= 1451617260: # 1451617260 = 2016-01-01\n",
    "        obj['date'] = time.strftime('%Y-%m-%d', time.localtime(date))\n",
    "        obj['text'] = re.sub(r'\\\\', '', obj['text'])\n",
    "        if 'extended_tweet' in obj: \n",
    "            obj['extended_tweet'] = re.sub(r'\\\\', '', obj['extended_tweet']['full_text'])\n",
    "        tweets_list.append(obj)\n",
    "\n",
    "    count += 1\n",
    "    if count % (int(file_len/1000)) == 0:\n",
    "        \n",
    "        #here I reset the list to save memory usage\n",
    "        if tweets_list: data = data.append(tweets_list, ignore_index=True)\n",
    "        tweets_list = []\n",
    "        \n",
    "        clear_output()\n",
    "        \n",
    "        frac = count/file_len*100\n",
    "        print(\"%.1f\" % frac, \"% done\", sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data['text'].apply(lambda x: tknzr.tokenize(x.translate(str.maketrans('', '', string.punctuation)).lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "VT = list(data['tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# continue"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "eng = create_engine(\"sqlite:///minha_tabela.sqlite\")\n",
    "\n",
    "class DHBBCorpus:\n",
    "    def __init__(self, ndocs=10000):\n",
    "        self.ndocs = min(7687,ndocs)\n",
    "        self.counter = 1\n",
    "    def __iter__(self):\n",
    "        with eng.connect() as con:\n",
    "            res = con.execute(f'select corpo from resultados limit {self.ndocs};')\n",
    "            for doc in res:\n",
    "                d = self.pre_process(doc[0])\n",
    "                if self.counter%10 == 0:\n",
    "                    print (f\"Verbete {self.counter} de {6*self.ndocs}\\r\", end='')\n",
    "                for s in d:\n",
    "                    yield s\n",
    "                self.counter += 1\n",
    "    def pre_process(self, doc):\n",
    "        n = nlp(doc, disable=['tagger', 'ner','entity-linker', 'textcat','entity-ruler','merge-noun-chunks','merge-entities','merge-subtokens'])\n",
    "        results = []\n",
    "        for sentence in n.sents:\n",
    "            s = sentence.text.split()\n",
    "            if not s:\n",
    "                continue\n",
    "            results.append([token.strip().strip(punctuation) for token in s if token.strip().strip(punctuation)])\n",
    "        return results\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo um pequeno exemplo de como a classe `DHBBCorpus` funciona:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['«José', 'Machado', 'Coelho', 'de', 'Castro»', 'nasceu', 'em', 'Lorena', 'SP']\n",
      "['Estudou', 'no']\n",
      "['Ginásio', 'Diocesano', 'de', 'São', 'Paulo', 'e', 'bacharelou-se', 'em', '1910']\n",
      "['pela', 'Faculdade', 'de', 'Ciências', 'Jurídicas', 'e', 'Sociais']\n",
      "['Dedicando-se']\n",
      "['à', 'advocacia']\n",
      "['foi', 'promotor', 'público', 'em', 'Cunha', 'SP', 'e', 'depois', 'delegado', 'de', 'polícia', 'no']\n",
      "['Rio', 'de', 'Janeiro', 'então', 'Distrito', 'Federal']\n",
      "['Iniciou', 'sua', 'vida', 'política', 'como', 'deputado', 'federal', 'pelo', 'Distrito', 'Federal', 'exercendo', 'o', 'mandato', 'de', '1927', 'a', '1929']\n",
      "['Reeleito', 'para', 'a', 'legislatura', 'iniciada', 'em', 'maio', 'de', '1930', 'ocupava', 'sua', 'cadeira', 'na', 'Câmara', 'quando', 'em', '3', 'de', 'outubro', 'foi', 'deflagrado', 'o', 'movimento', 'revolucionário', 'liderado', 'por', 'Getúlio', 'Vargas']\n"
     ]
    }
   ],
   "source": [
    "DC = DHBBCorpus(5)\n",
    "count = 0\n",
    "for f in DC:\n",
    "    count +=1\n",
    "    #pass\n",
    "    print(f)\n",
    "    if count == 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec \n",
    "Vamos começar pelo treinamento de um modelo word2vec. Este modelo itera 6 vezez sobre o corpus logo, devemos ver o contador atingir 46122. Estas repetições são necessárias para permitir a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('virus_tweets.w2v'):\n",
    "    model = Word2Vec.load('virus_tweets.w2v')\n",
    "else:\n",
    "    #DC = DHBBCorpus()\n",
    "    model = Word2Vec(sentences=VT, workers=32)\n",
    "    model.save('virus_tweets.w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "musica\n",
      "dos\n",
      "vizinhos\n",
      "zika\n",
      "aq\n",
      "da\n",
      "comunidade\n",
      "sei\n",
      "que\n",
      "estou\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(model.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3886912,\n",
       " 0.28217658,\n",
       " -1.9874773,\n",
       " -0.51558894,\n",
       " 2.1879008,\n",
       " -0.19042288,\n",
       " 0.6908539,\n",
       " -1.0632348,\n",
       " -0.67359257,\n",
       " 3.0175622,\n",
       " 2.2130032,\n",
       " -1.4433424,\n",
       " 2.760831,\n",
       " -2.1444695,\n",
       " -0.2821184,\n",
       " 0.58656615,\n",
       " 1.0061878,\n",
       " 1.1807309,\n",
       " 0.54593456,\n",
       " -2.3770607,\n",
       " 0.43160978,\n",
       " 0.050366312,\n",
       " 0.8009387,\n",
       " -1.0630683,\n",
       " -0.29855117,\n",
       " 1.0814375,\n",
       " 0.6338707,\n",
       " -0.58640933,\n",
       " -3.4926078,\n",
       " 2.4008555,\n",
       " -0.5238663,\n",
       " -0.7037424,\n",
       " 0.7415925,\n",
       " -0.30719492,\n",
       " 0.9929472,\n",
       " 5.1566324,\n",
       " 0.24720924,\n",
       " -3.5077336,\n",
       " -2.0185099,\n",
       " -1.5160787,\n",
       " -0.6066206,\n",
       " 1.5647931,\n",
       " 2.6941617,\n",
       " -2.4217505,\n",
       " 3.4705436,\n",
       " -0.01701793,\n",
       " 0.87891155,\n",
       " 1.8238404,\n",
       " -1.3465565,\n",
       " -1.3153664,\n",
       " 2.1167681,\n",
       " -0.6209916,\n",
       " -3.9595716,\n",
       " -1.1521416,\n",
       " -2.2909615,\n",
       " -3.2971458,\n",
       " 2.292861,\n",
       " 0.5738925,\n",
       " -2.6859908,\n",
       " -0.5351569,\n",
       " 2.9647572,\n",
       " -0.8235569,\n",
       " 1.3786242,\n",
       " -2.7816575,\n",
       " 1.4543685,\n",
       " -2.0188134,\n",
       " 1.541914,\n",
       " -1.7891498,\n",
       " 0.09042452,\n",
       " 1.3875531,\n",
       " 3.1705196,\n",
       " 1.4111038,\n",
       " 0.6391565,\n",
       " 1.4406295,\n",
       " 4.193468,\n",
       " -0.047331657,\n",
       " -2.1729064,\n",
       " -0.7105255,\n",
       " 0.08725463,\n",
       " -2.4469893,\n",
       " -1.7206128,\n",
       " -3.0422556,\n",
       " 0.80763227,\n",
       " 0.08858246,\n",
       " 2.578868,\n",
       " -0.5694333,\n",
       " 2.7730608,\n",
       " 2.600093,\n",
       " 1.1647948,\n",
       " 0.9072937,\n",
       " -0.5685364,\n",
       " -1.0985816,\n",
       " 2.4794917,\n",
       " 1.4495918,\n",
       " -2.2186701,\n",
       " -0.36773166,\n",
       " -1.0040663,\n",
       " 2.3468719,\n",
       " 1.7736709,\n",
       " -0.8769256]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.wv['deputado'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179911"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aedes', 0.6367322206497192),\n",
       " ('denguezika', 0.6295568943023682),\n",
       " ('de', 0.6234614849090576),\n",
       " ('virus', 0.609860360622406),\n",
       " ('zika', 0.5731625556945801),\n",
       " ('que', 0.5616250038146973),\n",
       " ('a', 0.5592842102050781),\n",
       " ('rt', 0.5592179298400879),\n",
       " ('e', 0.5446224212646484),\n",
       " ('o', 0.5232881307601929),\n",
       " ('zikadengue', 0.5191036462783813),\n",
       " ('no', 0.5125505924224854),\n",
       " ('nao', 0.5100154876708984),\n",
       " ('doenca', 0.5021892189979553),\n",
       " ('se', 0.49738809466362),\n",
       " ('ja', 0.4926435947418213),\n",
       " ('mas', 0.48960384726524353),\n",
       " ('aegypti', 0.47452616691589355),\n",
       " ('httpstcobvxuoriibu', 0.4638242721557617),\n",
       " ('la', 0.4629664421081543)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['dengue'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60306788"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
