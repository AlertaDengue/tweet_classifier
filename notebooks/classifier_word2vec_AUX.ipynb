{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerador de modelo Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, pickle\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import string\n",
    "from string import punctuation\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from gensim.models import Word2Vec, word2vec\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = '../outputs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrieve data from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "import dns\n",
    "client = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.twitter\n",
    "collection_ufmg = db.ufmg_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0% done\n"
     ]
    }
   ],
   "source": [
    "file_len = 7503436\n",
    "objects = collection_ufmg.find({})\n",
    "tweets_list = []\n",
    "data = pd.DataFrame()\n",
    "count = 0\n",
    "for obj in objects:\n",
    "    date = obj['date']\n",
    "    if isinstance(date, int):\n",
    "        date = date/1000\n",
    "    else: date = time.mktime(time.strptime(date[:10], '%Y-%m-%d'))\n",
    "    #if date >= 1451617260: # 1451617260 = 2016-01-01\n",
    "    obj['date'] = time.strftime('%Y-%m-%d', time.localtime(date))\n",
    "    obj['text'] = re.sub(r'\\\\', '', obj['text'])\n",
    "    if 'extended_tweet' in obj: \n",
    "        obj['extended_tweet'] = re.sub(r'\\\\', '', obj['extended_tweet']['full_text'])\n",
    "    tweets_list.append(obj)\n",
    "\n",
    "    count += 1\n",
    "    if count % (int(file_len/1000)) == 0:\n",
    "        \n",
    "        #here I reset the list to save memory usage\n",
    "        if tweets_list: data = data.append(tweets_list, ignore_index=True)\n",
    "        tweets_list = []\n",
    "        \n",
    "        clear_output()\n",
    "        \n",
    "        frac = count/file_len*100\n",
    "        print(\"%.1f\" % frac, \"% done\", sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clear text"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_bak = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing regex\n",
    "text = 'estou com dengue, estou mesmo, nada restou'\n",
    "text = re.sub('(^|\\W)estou', r'\\1to', text)\n",
    "text = re.sub('(\\W)com(\\W)', r'\\1cm\\2', text)\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    text = re.sub('a+', 'a', text)\n",
    "    text = re.sub('e+', 'e', text)\n",
    "    text = re.sub('i+', 'i', text)\n",
    "    text = re.sub('o+', 'o', text)\n",
    "    text = re.sub('u+', 'u', text)\n",
    "    text = re.sub('(^|\\W)estou', r'\\1to', text)\n",
    "    text = re.sub('(\\W)com(\\W)', r'\\1cm\\2', text)\n",
    "    text = re.sub('(\\W)muito(\\W)', r'\\1mto\\2', text)\n",
    "    #text = re.sub('https?\\:\\/\\/[\\w\\.\\/\\?\\\\]+', '', text) # remove URLs, omitted to avoid errors\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_char(text):\n",
    "    text = re.sub('[áàãâ]', 'a', text)\n",
    "    text = re.sub('[óòõô]', 'o', text)\n",
    "    text = re.sub('[éèê]', 'e', text)\n",
    "    text = re.sub('[íì]', 'i', text)\n",
    "    text = re.sub('[úù]', 'u', text)\n",
    "    text = re.sub('ç', 'c', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tokens(text):\n",
    "    if text is not None: tokens = tknzr.tokenize(text.translate(str.maketrans('', '', string.punctuation)))\n",
    "    else: tokens = []\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_original'] = data['text']\n",
    "data['text'] = data['text'].apply(lambda x: clear_text(x.lower()))\n",
    "data['tokens'] = data['text'].apply(lambda x: build_tokens(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build tokens for encoded text (just removed special characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_encoded'] = data['text'].apply(lambda x: remove_special_char(x))\n",
    "data['tokens_encoded'] = data['text_encoded'].apply(lambda x: build_tokens(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### texts to variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "VT = list(data['tokens'])\n",
    "VT_encoded = list(data['tokens_encoded'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build tokens and variable with bigrams\n",
    "* source: https://radimrehurek.com/gensim/models/phrases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(data['tokens_encoded'])\n",
    "phrases = Phrases(sentences, min_count=10, threshold=1)\n",
    "\n",
    "VT_encoded_bigrams = []\n",
    "bigram = Phraser(phrases)\n",
    "for sent in bigram[sentences]:\n",
    "    VT_encoded_bigrams.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['musica', 'dos_vizinhos', 'zika', 'aq', 'da_comunidade'], ['sei_que', 'estou', 'atrasada', 'mas', 'feliz_ano', 'novo', 'que', 'esse_ano', 'seja', 'zika', 'para', 'todos_nos', 'ask_5sosfam'], ['o', 'ano_ja', 'comeca_assim', 'eu', 'em_casa', 'com_suspeita', 'de', 'dengue'], ['vitoriawg', 'imagina_qnd', 'a', 'lua', 'entrar_em', 'aquario', 'eu_vo', 'vira', 'de', 'humanas', 'vo', 'aplaudi_o', 'sol', 'faze_uma', 'tatto', 'iludi', 'os_boy', 'vai', 'se', 'zika_virus'], ['o', 'mais', 'zika', 'e', 'quem', 'tem_palavra', 'e', 'nao', 'que_vende', 'po']]\n"
     ]
    }
   ],
   "source": [
    "print(VT_encoded_bigrams[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save collocation model.\n",
    "bigram.save(os.path.join(outputs, \"bigram_model.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#v1 = virus_tweets, v2 = virus_tweets_encoded, v3 = virus_tweets_encoded_bigrams\n",
    "for file, var_list in zip(['virus_tweets.w2v', 'virus_tweets_encoded.w2v', 'virus_tweets_encoded_bigrams.w2v'], \n",
    "                          [VT, VT_encoded, VT_encoded_bigrams]):\n",
    "    model = Word2Vec(sentences=var_list, workers=32)\n",
    "    model.save(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'virus_tweets.w2v'\n",
    "model = Word2Vec.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "musica\n",
      "dos\n",
      "vizinhos\n",
      "zika\n",
      "aq\n",
      "da\n",
      "comunidade\n",
      "sei\n",
      "que\n",
      "estou\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(model.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3886912,\n",
       " 0.28217658,\n",
       " -1.9874773,\n",
       " -0.51558894,\n",
       " 2.1879008,\n",
       " -0.19042288,\n",
       " 0.6908539,\n",
       " -1.0632348,\n",
       " -0.67359257,\n",
       " 3.0175622,\n",
       " 2.2130032,\n",
       " -1.4433424,\n",
       " 2.760831,\n",
       " -2.1444695,\n",
       " -0.2821184,\n",
       " 0.58656615,\n",
       " 1.0061878,\n",
       " 1.1807309,\n",
       " 0.54593456,\n",
       " -2.3770607,\n",
       " 0.43160978,\n",
       " 0.050366312,\n",
       " 0.8009387,\n",
       " -1.0630683,\n",
       " -0.29855117,\n",
       " 1.0814375,\n",
       " 0.6338707,\n",
       " -0.58640933,\n",
       " -3.4926078,\n",
       " 2.4008555,\n",
       " -0.5238663,\n",
       " -0.7037424,\n",
       " 0.7415925,\n",
       " -0.30719492,\n",
       " 0.9929472,\n",
       " 5.1566324,\n",
       " 0.24720924,\n",
       " -3.5077336,\n",
       " -2.0185099,\n",
       " -1.5160787,\n",
       " -0.6066206,\n",
       " 1.5647931,\n",
       " 2.6941617,\n",
       " -2.4217505,\n",
       " 3.4705436,\n",
       " -0.01701793,\n",
       " 0.87891155,\n",
       " 1.8238404,\n",
       " -1.3465565,\n",
       " -1.3153664,\n",
       " 2.1167681,\n",
       " -0.6209916,\n",
       " -3.9595716,\n",
       " -1.1521416,\n",
       " -2.2909615,\n",
       " -3.2971458,\n",
       " 2.292861,\n",
       " 0.5738925,\n",
       " -2.6859908,\n",
       " -0.5351569,\n",
       " 2.9647572,\n",
       " -0.8235569,\n",
       " 1.3786242,\n",
       " -2.7816575,\n",
       " 1.4543685,\n",
       " -2.0188134,\n",
       " 1.541914,\n",
       " -1.7891498,\n",
       " 0.09042452,\n",
       " 1.3875531,\n",
       " 3.1705196,\n",
       " 1.4111038,\n",
       " 0.6391565,\n",
       " 1.4406295,\n",
       " 4.193468,\n",
       " -0.047331657,\n",
       " -2.1729064,\n",
       " -0.7105255,\n",
       " 0.08725463,\n",
       " -2.4469893,\n",
       " -1.7206128,\n",
       " -3.0422556,\n",
       " 0.80763227,\n",
       " 0.08858246,\n",
       " 2.578868,\n",
       " -0.5694333,\n",
       " 2.7730608,\n",
       " 2.600093,\n",
       " 1.1647948,\n",
       " 0.9072937,\n",
       " -0.5685364,\n",
       " -1.0985816,\n",
       " 2.4794917,\n",
       " 1.4495918,\n",
       " -2.2186701,\n",
       " -0.36773166,\n",
       " -1.0040663,\n",
       " 2.3468719,\n",
       " 1.7736709,\n",
       " -0.8769256]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list(model.wv['dengue']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179911"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('aedes', 0.6367322206497192),\n",
       " ('denguezika', 0.6295568943023682),\n",
       " ('de', 0.6234614849090576),\n",
       " ('virus', 0.609860360622406),\n",
       " ('zika', 0.5731625556945801),\n",
       " ('que', 0.5616250038146973),\n",
       " ('a', 0.5592842102050781),\n",
       " ('rt', 0.5592179298400879),\n",
       " ('e', 0.5446224212646484),\n",
       " ('o', 0.5232881307601929),\n",
       " ('zikadengue', 0.5191036462783813),\n",
       " ('no', 0.5125505924224854),\n",
       " ('nao', 0.5100154876708984),\n",
       " ('doenca', 0.5021892189979553),\n",
       " ('se', 0.49738809466362),\n",
       " ('ja', 0.4926435947418213),\n",
       " ('mas', 0.48960384726524353),\n",
       " ('aegypti', 0.47452616691589355),\n",
       " ('httpstcobvxuoriibu', 0.4638242721557617),\n",
       " ('la', 0.4629664421081543)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['dengue'], topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60306788"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.corpus_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
